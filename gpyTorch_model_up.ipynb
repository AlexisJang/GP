{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SX4b8gLC6Bl8",
    "outputId": "ffb6fb31-713d-44c3-8708-33f8a767a6a0"
   },
   "outputs": [],
   "source": [
    "# INSTALL PACKAGES\n",
    "# !pip install gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0Bae7k8VVGeh"
   },
   "outputs": [],
   "source": [
    "# LOAD PACKAGES\n",
    "import warnings\n",
    "import torch\n",
    "from gpytorch.distributions import base_distributions\n",
    "from gpytorch.functions import log_normal_cdf\n",
    "from gpytorch.likelihoods.likelihood import _OneDimensionalLikelihood\n",
    "from gpytorch.constraints import Positive\n",
    "from gpytorch.kernels import Kernel, ScaleKernel, RBFKernel\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "import gpytorch\n",
    "import time as time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLASS DEFINITION AND INITIALIZATION\n",
    "class BinomialLikelihood(_OneDimensionalLikelihood):\n",
    "    r\"\"\"\n",
    "    Implements the Binomial likelihood for count data y between 1 and m.\n",
    "        The Binomial distribution is parameterized by :math:`m > 0`.\n",
    "    We can write the likelihood as:\n",
    "\n",
    "    .. math::\n",
    "        \\begin{equation*}\n",
    "            p(Y=y|f,m)=\\phi(f)^y(1-\\phi(f))^{(m-y)}\n",
    "        \\end{equation*}\n",
    "    \"\"\"\n",
    "\n",
    "# initialize the likelihood with the number of trials\n",
    "    def __init__(self, n_trials):\n",
    "        super().__init__()\n",
    "        self.n_trials = n_trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a new class named `BinomialLilihood`, which inherits from `_OneDimensionalLikelihood`, a base class for one-dimensional likelihoods. \n",
    "\n",
    "We then define a initialization method with the parameter `n_trials`. Recall that our number of trials `m` is an integer greater than 0. We store the number of trials in the variable `self.n_trials`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FORWARD \n",
    "# transforms function samples into Binomial distribution parameters\n",
    "    def forward(self, function_samples, **kwargs):\n",
    "        output_probs = base_distributions.Normal(0, 1).cdf(function_samples)\n",
    "        print(output_probs.size())\n",
    "        \n",
    "        return base_distributions.Binomial(total_count=self.n_trials, probs=output_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The function `forward` computes the likelihood given `function_samples`. The parameter `function_samples` samples from the latent function `f`. `**kwargs` represents additional keyword arguments. We first compute the CDF of a normal distribution by creating a standard normal distribution (`mean=0`, `variance=1`) and computing the CDF at the points specified by `function_samples`. This transforms `function_samples` into probabilities between 0 and 1, which is suitable for the Binomial likelihood's `probs` parameter. To ensure correct dimensionality, we print the shape (size) of the `output_probs` tensor. The function returns a binomial distribution object with 1) `total_count`, a set to the number of trials `self.n_trials`; and 2) `probs`, a set to the computed probabilities from the normal CDF (`output_probs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOG MARGINAL \n",
    "# computes the log probability of observations under the marginal distribution\n",
    "    def log_marginal(self, observations, function_dist, *args, **kwargs):\n",
    "        marginal = self.marginal(function_dist, *args, **kwargs)\n",
    "        return marginal.log_prob(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `log_marginal` computes the logarithm of the marginal probability of the observations given the function distribution. The parameters 1) `observations` represents the observed count data; 2) `function_dist` indicates the distribution over the latent function `f`; and 3) `*args, **kwargs` represent additional positional and keyword arguments. \n",
    "\n",
    "The function calls the `marginal` to compute the marginal distribution based on `function_dist` and any additional arguments. It then returns the log probability of the `observations` under the computed `marginal` distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MARGINAL\n",
    "# constructs the marginal Binomial distribution based on the function distribution's \n",
    "# mean and variance\n",
    "    def marginal(self, function_dist, **kwargs):\n",
    "        mean = function_dist.mean\n",
    "        var = function_dist.variance\n",
    "        link = mean.div(torch.sqrt(1 + var))\n",
    "        output_probs = base_distributions.Normal(0, 1).cdf(link)\n",
    "        return base_distributions.Binomial(total_count=self.num_data, probs=output_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`marginal` computes the marginal distribution based on the function distribution. After extracting the mean and variance of the `function_dist`, it computes the link function by scaling the mean with the square root of `(1 + variance)`. The purpose of this transformation is to map the function distribution's parameters to the probability space suitable for the Binomial distribution. Similar to the `forward` function defined above, `output_probs` computes the CDF of a standard normal distribution at the transformed `link` values, which gives us probabilities between 0 and 1. The function returns a binomial distribution with 1) `total_count`, a set to `self.num_data`; and 2) `probs`, a set to the computed probabilities `output_probs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPECTED LOG PROBABILITY\n",
    "# calculates the expected log probability of observations\n",
    "    def expected_log_prob(self, observations, function_dist, *params, **kwargs):\n",
    "        if torch.any(torch.logical_or(observations.le(-1), observations.ge(self.n_trials+1))):\n",
    "            warnings.warn(\n",
    "                \"BinomialLikelihood.expected_log_prob expects observations with labels in [0, m]. \"\n",
    "                \"Observations <0 or >m are not allowed.\",\n",
    "                DeprecationWarning,\n",
    "            )\n",
    "        else:\n",
    "            for i in range(observations.size(0)):\n",
    "                observations[i] = torch.clamp(observations[i],0,self.n_trials[i])\n",
    "\n",
    "        log_prob_lambda = lambda function_samples: self.n_trials*log_normal_cdf(-function_samples) + \\\n",
    "                observations.mul(log_normal_cdf(function_samples)-log_normal_cdf(-function_samples))\n",
    "        \n",
    "        log_prob = self.quadrature(log_prob_lambda, function_dist)\n",
    "        \n",
    "        return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `expected_log_prob` computes the expected log probability of the observations given the function distribution. The `if` statement checks if any observations are less than or equal to -1, or greater than or equal to `self.n_trials + 1`. This is to ensure that all observations lie within the valid range [0, m] for a binomial distribution. If any observations fall outside the valid range (exceeding the number of trials or being negative), we get a deprecation warning. The function iterates over each observation and clamps its value and make sure it lies witin `[0, self.n_trials[i]]`. \n",
    "\n",
    "We also define an inline log probability lambda function that calculates the log probability based on `function_samples`. `self.n_trials * log_normal_cdf(-function_samples)` computes the contribution from the failures and `observations.mul(log_normal_cdf(function_samples) - log_normal_cdf(-function_samples))` computes the contribution from successes. Note that `log_normal_cdf` is a function that computes the logarithm of the normal CDF.\n",
    "\n",
    "Next, `lob_prob` calls the `quadrature` to numerically integrate the `log_prob_lambda` over the `function_dist`. The purpose of this line is to compute the expected log probability by integrating over the uncertainty in the function distribution. \n",
    "\n",
    "The function returns the computed expected log probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q5xTpwZ4VJoP"
   },
   "outputs": [],
   "source": [
    "# # implement Binomial Likelihood\n",
    "# class BinomialLikelihood(_OneDimensionalLikelihood):\n",
    "#     r\"\"\"\n",
    "#     Implements the Binomial likelihood for count data y between 1 and m.\n",
    "#         The Binomial distribution is parameterized by :math:`m > 0`.\n",
    "#     We can write the likelihood as:\n",
    "\n",
    "#     .. math::\n",
    "#         \\begin{equation*}\n",
    "#             p(Y=y|f,m)=\\phi(f)^y(1-\\phi(f))^{(m-y)}\n",
    "#         \\end{equation*}\n",
    "#     \"\"\"\n",
    "\n",
    "# # calculate total number of trials in the Binomial distribution\n",
    "#     def __init__(self, n_trials):\n",
    "#         super().__init__()\n",
    "#         self.n_trials = n_trials\n",
    "\n",
    "# # calculate CDF of a normal distribution with mean 0 and variance 1 at the 'function samples'\n",
    "#     def forward(self, function_samples, **kwargs):\n",
    "#         output_probs = base_distributions.Normal(0, 1).cdf(function_samples)\n",
    "#         print(output_probs.size())\n",
    "        \n",
    "#         return base_distributions.Binomial(total_count=self.n_trials, probs=output_probs)\n",
    "\n",
    "# # calculate the log marginal probabilities given the function distribution\n",
    "#     def log_marginal(self, observations, function_dist, *args, **kwargs):\n",
    "#         marginal = self.marginal(function_dist, *args, **kwargs)\n",
    "#         return marginal.log_prob(observations)\n",
    "\n",
    "# # compute the marginal distribution based on the mean and variance\n",
    "#     def marginal(self, function_dist, **kwargs):\n",
    "#         mean = function_dist.mean\n",
    "#         var = function_dist.variance\n",
    "#         link = mean.div(torch.sqrt(1 + var))\n",
    "#         output_probs = base_distributions.Normal(0, 1).cdf(link)\n",
    "#         return base_distributions.Binomial(total_count=self.num_data, probs=output_probs)\n",
    "\n",
    "# # calculate the expected log probability of observation given the function distribution\n",
    "#     def expected_log_prob(self, observations, function_dist, *params, **kwargs):\n",
    "#         if torch.any(torch.logical_or(observations.le(-1), observations.ge(self.n_trials+1))):\n",
    "#             # Remove after 1.0\n",
    "#             warnings.warn(\n",
    "#                 \"BinomialLikelihood.expected_log_prob expects observations with labels in [0, m]. \"\n",
    "#                 \"Observations <0 or >m are not allowed.\",\n",
    "#                 DeprecationWarning,\n",
    "#             )\n",
    "#         else:\n",
    "#             for i in range(observations.size(0)):\n",
    "#                 observations[i] = torch.clamp(observations[i],0,self.n_trials[i])\n",
    "\n",
    "#         log_prob_lambda = lambda function_samples: self.n_trials*log_normal_cdf(-function_samples) + \\\n",
    "#                 observations.mul(log_normal_cdf(function_samples)-log_normal_cdf(-function_samples))\n",
    "#         log_prob = self.quadrature(log_prob_lambda, function_dist)\n",
    "#         return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9mQyzD5tWloa"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENT GP CLASS\n",
    "gpytorch.settings.cholesky_jitter.value = 1e-4\n",
    "\n",
    "class BinomialGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, train_x):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self, train_x, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(BinomialGPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.LinearMean(input_size=train_x.size(1))\n",
    "        # ARD kernel for covariate, geospatial and time confounding\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(ard_num_dims=train_x.size(1)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first add a small jitter to the diagonal of the covariance matrix during Cholesky decomposition. Cholesky decomposition can fail if the covariance matrix is not positive definite. The jitter mitigates this concern by making the matrix strictly positive definite. \n",
    "\n",
    "The `BinomialGPModel` class implements a GP model tailored for binomial likelihoods. It inherits from `gpytorch.models.ApproximateGP` that enables the use of variational inference for scalable GP models. \n",
    "\n",
    "`def __init__(self, train_x)` initializes the GP model with training data. `train_x` represents the input training data, which is a tensor of shape `(n_samples, n_features)`. `variational_distribution` defines the variational distribution that approximates the posterior over the latent function `f`. `CholeskyVariationalDistribution` represents a multivariate Gaussian distribution parameterized by a mean vector and a lower-triangular matrix (Cholesky  factor) for the covariance. `train_x.size(0)` specifies the number of variational parameters based on the number of training samples. \n",
    "\n",
    "`variational_strategy` defines the strategy for variational inference by specifying how the variational distribution interacts with the GP model. `self` refers to the current GP model instance; `train_x` specifies the input of training data; `variational_distribution` represents the variational distribution defined earlier; and `learn_inducing_locations=False` indicates whether the inducing points' locations should be learned during training. By setting it to `False`, we keep them fixed. \n",
    "\n",
    "We initialize the superclass `ApproximateGP` with the defined variational strategy. \n",
    "\n",
    "`self.mean_module` defines the mean function of the GP. `LinearMean` is a linear mean function that models the expected value of the latent function `f` as a linear combination of the input features. `input_size=train_x.size(1)` sets the number of input features based on the training data. \n",
    "\n",
    "`self.cover_module` defines the covariance (kernel) function of the GP. `ard_num_dims=train_x.size(1)` enables Automatic Relevance Determine (ARD) by assigning a separate lengthscale parameters to each input dimension. This allows the model to weigh the importance of each feature. Radial Basis Function (RBF) kernel measures similarity based on the distance between inputs, and Scale Kernel scales the output of the RBF kernel by a learned scalar factor to allow the model to adjust the overall variance. \n",
    "\n",
    "Finally, the `forward` defines the forward pass of the GP model that maps the inputs to a multivariate normal distribution over outputs. `x` is input data for which GP predictions are to be made. Using this, we compute the mean and covariance of the latent function `f`. We apply the linear mean function defined during initialization to the input `x` to compute the mean. This results in a mean vector of shape `(n_samples,)`. When computing the covariance matrix, we apply the scaled RBF kernel with ARD to the input `x`, resulting in a covariance matrix of shape `(n_samples, n_samples)`. \n",
    "\n",
    "The function returns a multivariate normal distribution representing the GP's predictions for the input `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "R-FM23l6K_wf"
   },
   "outputs": [],
   "source": [
    "def transform_data(data):\n",
    "    n = data.shape[0] # number of samples in the dataset\n",
    "    x = np.zeros((n, 6)) # initialize an input matrix with 6 columns\n",
    "\n",
    "    # initialize target variables\n",
    "    y = np.zeros((n,)) \n",
    "    theta = np.zeros((n,))\n",
    "    N = np.zeros((n,))\n",
    "\n",
    "    # encode gender as binary variables\n",
    "    x[:,0] = (data[\"gender\"] == \"Male\")\n",
    "    x[:,1] = (data[\"gender\"] == \"Female\")\n",
    "\n",
    "    # encode race as binary variables\n",
    "    x[:,2] = (data[\"race\"] == \"White\")\n",
    "    x[:,3] = (data[\"race\"] == \"Black\")\n",
    "    x[:,4] = (data[\"race\"] == \"Other\")\n",
    "\n",
    "    # year as a continuous variable\n",
    "    x[:,5] = data[\"year\"].to_numpy()\n",
    "\n",
    "    # extract target variables\n",
    "    theta = data[\"theta\"].to_numpy()\n",
    "    y = data[\"Y\"].to_numpy()\n",
    "    N = data[\"n\"].to_numpy()\n",
    "    \n",
    "    return torch.from_numpy(x).float(), torch.from_numpy(y).float(),\\\n",
    "       torch.from_numpy(N).float(), torch.from_numpy(theta).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/200 - Loss: 171.934\n",
      "Iter 2/200 - Loss: 167.304\n",
      "Iter 3/200 - Loss: 162.773\n",
      "Iter 4/200 - Loss: 158.441\n",
      "Iter 5/200 - Loss: 154.287\n",
      "Iter 6/200 - Loss: 150.278\n",
      "Iter 7/200 - Loss: 146.417\n",
      "Iter 8/200 - Loss: 142.721\n",
      "Iter 9/200 - Loss: 139.195\n",
      "Iter 10/200 - Loss: 135.825\n",
      "Iter 11/200 - Loss: 132.609\n",
      "Iter 12/200 - Loss: 129.556\n",
      "Iter 13/200 - Loss: 126.670\n",
      "Iter 14/200 - Loss: 123.945\n",
      "Iter 15/200 - Loss: 121.371\n",
      "Iter 16/200 - Loss: 118.944\n",
      "Iter 17/200 - Loss: 116.665\n",
      "Iter 18/200 - Loss: 114.529\n",
      "Iter 19/200 - Loss: 112.531\n",
      "Iter 20/200 - Loss: 110.662\n",
      "Iter 21/200 - Loss: 108.923\n",
      "Iter 22/200 - Loss: 107.305\n",
      "Iter 23/200 - Loss: 105.804\n",
      "Iter 24/200 - Loss: 104.414\n",
      "Iter 25/200 - Loss: 103.128\n",
      "Iter 26/200 - Loss: 101.940\n",
      "Iter 27/200 - Loss: 100.841\n",
      "Iter 28/200 - Loss: 99.828\n",
      "Iter 29/200 - Loss: 98.896\n",
      "Iter 30/200 - Loss: 98.037\n",
      "Iter 31/200 - Loss: 97.246\n",
      "Iter 32/200 - Loss: 96.517\n",
      "Iter 33/200 - Loss: 95.845\n",
      "Iter 34/200 - Loss: 95.224\n",
      "Iter 35/200 - Loss: 94.652\n",
      "Iter 36/200 - Loss: 94.119\n",
      "Iter 37/200 - Loss: 93.625\n",
      "Iter 38/200 - Loss: 93.164\n",
      "Iter 39/200 - Loss: 92.731\n",
      "Iter 40/200 - Loss: 92.325\n",
      "Iter 41/200 - Loss: 91.943\n",
      "Iter 42/200 - Loss: 91.579\n",
      "Iter 43/200 - Loss: 91.235\n",
      "Iter 44/200 - Loss: 90.907\n",
      "Iter 45/200 - Loss: 90.591\n",
      "Iter 46/200 - Loss: 90.289\n",
      "Iter 47/200 - Loss: 89.997\n",
      "Iter 48/200 - Loss: 89.715\n",
      "Iter 49/200 - Loss: 89.442\n",
      "Iter 50/200 - Loss: 89.179\n",
      "Iter 51/200 - Loss: 88.922\n",
      "Iter 52/200 - Loss: 88.674\n",
      "Iter 53/200 - Loss: 88.433\n",
      "Iter 54/200 - Loss: 88.199\n",
      "Iter 55/200 - Loss: 87.971\n",
      "Iter 56/200 - Loss: 87.751\n",
      "Iter 57/200 - Loss: 87.537\n",
      "Iter 58/200 - Loss: 87.330\n",
      "Iter 59/200 - Loss: 87.130\n",
      "Iter 60/200 - Loss: 86.936\n",
      "Iter 61/200 - Loss: 86.747\n",
      "Iter 62/200 - Loss: 86.567\n",
      "Iter 63/200 - Loss: 86.392\n",
      "Iter 64/200 - Loss: 86.223\n",
      "Iter 65/200 - Loss: 86.060\n",
      "Iter 66/200 - Loss: 85.903\n",
      "Iter 67/200 - Loss: 85.752\n",
      "Iter 68/200 - Loss: 85.607\n",
      "Iter 69/200 - Loss: 85.466\n",
      "Iter 70/200 - Loss: 85.330\n",
      "Iter 71/200 - Loss: 85.200\n",
      "Iter 72/200 - Loss: 85.074\n",
      "Iter 73/200 - Loss: 84.952\n",
      "Iter 74/200 - Loss: 84.835\n",
      "Iter 75/200 - Loss: 84.721\n",
      "Iter 76/200 - Loss: 84.611\n",
      "Iter 77/200 - Loss: 84.504\n",
      "Iter 78/200 - Loss: 84.401\n",
      "Iter 79/200 - Loss: 84.300\n",
      "Iter 80/200 - Loss: 84.203\n",
      "Iter 81/200 - Loss: 84.108\n",
      "Iter 82/200 - Loss: 84.017\n",
      "Iter 83/200 - Loss: 83.927\n",
      "Iter 84/200 - Loss: 83.839\n",
      "Iter 85/200 - Loss: 83.754\n",
      "Iter 86/200 - Loss: 83.671\n",
      "Iter 87/200 - Loss: 83.591\n",
      "Iter 88/200 - Loss: 83.512\n",
      "Iter 89/200 - Loss: 83.435\n",
      "Iter 90/200 - Loss: 83.359\n",
      "Iter 91/200 - Loss: 83.285\n",
      "Iter 92/200 - Loss: 83.213\n",
      "Iter 93/200 - Loss: 83.143\n",
      "Iter 94/200 - Loss: 83.074\n",
      "Iter 95/200 - Loss: 83.006\n",
      "Iter 96/200 - Loss: 82.940\n",
      "Iter 97/200 - Loss: 82.876\n",
      "Iter 98/200 - Loss: 82.813\n",
      "Iter 99/200 - Loss: 82.751\n",
      "Iter 100/200 - Loss: 82.691\n",
      "Iter 101/200 - Loss: 82.631\n",
      "Iter 102/200 - Loss: 82.574\n",
      "Iter 103/200 - Loss: 82.517\n",
      "Iter 104/200 - Loss: 82.461\n",
      "Iter 105/200 - Loss: 82.406\n",
      "Iter 106/200 - Loss: 82.353\n",
      "Iter 107/200 - Loss: 82.300\n",
      "Iter 108/200 - Loss: 82.249\n",
      "Iter 109/200 - Loss: 82.198\n",
      "Iter 110/200 - Loss: 82.149\n",
      "Iter 111/200 - Loss: 82.100\n",
      "Iter 112/200 - Loss: 82.053\n",
      "Iter 113/200 - Loss: 82.006\n",
      "Iter 114/200 - Loss: 81.960\n",
      "Iter 115/200 - Loss: 81.915\n",
      "Iter 116/200 - Loss: 81.871\n",
      "Iter 117/200 - Loss: 81.828\n",
      "Iter 118/200 - Loss: 81.785\n",
      "Iter 119/200 - Loss: 81.743\n",
      "Iter 120/200 - Loss: 81.701\n",
      "Iter 121/200 - Loss: 81.661\n",
      "Iter 122/200 - Loss: 81.622\n",
      "Iter 123/200 - Loss: 81.583\n",
      "Iter 124/200 - Loss: 81.544\n",
      "Iter 125/200 - Loss: 81.507\n",
      "Iter 126/200 - Loss: 81.470\n",
      "Iter 127/200 - Loss: 81.433\n",
      "Iter 128/200 - Loss: 81.398\n",
      "Iter 129/200 - Loss: 81.362\n",
      "Iter 130/200 - Loss: 81.328\n",
      "Iter 131/200 - Loss: 81.294\n",
      "Iter 132/200 - Loss: 81.261\n",
      "Iter 133/200 - Loss: 81.228\n",
      "Iter 134/200 - Loss: 81.196\n",
      "Iter 135/200 - Loss: 81.164\n",
      "Iter 136/200 - Loss: 81.133\n",
      "Iter 137/200 - Loss: 81.102\n",
      "Iter 138/200 - Loss: 81.072\n",
      "Iter 139/200 - Loss: 81.042\n",
      "Iter 140/200 - Loss: 81.013\n",
      "Iter 141/200 - Loss: 80.984\n",
      "Iter 142/200 - Loss: 80.956\n",
      "Iter 143/200 - Loss: 80.928\n",
      "Iter 144/200 - Loss: 80.901\n",
      "Iter 145/200 - Loss: 80.874\n",
      "Iter 146/200 - Loss: 80.848\n",
      "Iter 147/200 - Loss: 80.822\n",
      "Iter 148/200 - Loss: 80.797\n",
      "Iter 149/200 - Loss: 80.772\n",
      "Iter 150/200 - Loss: 80.747\n",
      "Iter 151/200 - Loss: 80.723\n",
      "Iter 152/200 - Loss: 80.699\n",
      "Iter 153/200 - Loss: 80.675\n",
      "Iter 154/200 - Loss: 80.652\n",
      "Iter 155/200 - Loss: 80.630\n",
      "Iter 156/200 - Loss: 80.607\n",
      "Iter 157/200 - Loss: 80.585\n",
      "Iter 158/200 - Loss: 80.564\n",
      "Iter 159/200 - Loss: 80.543\n",
      "Iter 160/200 - Loss: 80.522\n",
      "Iter 161/200 - Loss: 80.501\n",
      "Iter 162/200 - Loss: 80.481\n",
      "Iter 163/200 - Loss: 80.461\n",
      "Iter 164/200 - Loss: 80.442\n",
      "Iter 165/200 - Loss: 80.422\n",
      "Iter 166/200 - Loss: 80.403\n",
      "Iter 167/200 - Loss: 80.385\n",
      "Iter 168/200 - Loss: 80.367\n",
      "Iter 169/200 - Loss: 80.349\n",
      "Iter 170/200 - Loss: 80.331\n",
      "Iter 171/200 - Loss: 80.314\n",
      "Iter 172/200 - Loss: 80.296\n",
      "Iter 173/200 - Loss: 80.280\n",
      "Iter 174/200 - Loss: 80.263\n",
      "Iter 175/200 - Loss: 80.247\n",
      "Iter 176/200 - Loss: 80.231\n",
      "Iter 177/200 - Loss: 80.215\n",
      "Iter 178/200 - Loss: 80.200\n",
      "Iter 179/200 - Loss: 80.185\n",
      "Iter 180/200 - Loss: 80.169\n",
      "Iter 181/200 - Loss: 80.155\n",
      "Iter 182/200 - Loss: 80.140\n",
      "Iter 183/200 - Loss: 80.126\n",
      "Iter 184/200 - Loss: 80.112\n",
      "Iter 185/200 - Loss: 80.098\n",
      "Iter 186/200 - Loss: 80.085\n",
      "Iter 187/200 - Loss: 80.072\n",
      "Iter 188/200 - Loss: 80.058\n",
      "Iter 189/200 - Loss: 80.045\n",
      "Iter 190/200 - Loss: 80.033\n",
      "Iter 191/200 - Loss: 80.020\n",
      "Iter 192/200 - Loss: 80.008\n",
      "Iter 193/200 - Loss: 79.996\n",
      "Iter 194/200 - Loss: 79.984\n",
      "Iter 195/200 - Loss: 79.972\n",
      "Iter 196/200 - Loss: 79.961\n",
      "Iter 197/200 - Loss: 79.949\n",
      "Iter 198/200 - Loss: 79.938\n",
      "Iter 199/200 - Loss: 79.927\n",
      "Iter 200/200 - Loss: 79.916\n",
      "Training completed in 61.09086346626282 seconds\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_data = pd.read_csv(\"../Dataset/superSurvey_clean_train_data.csv\", index_col=0)\n",
    "test_data = pd.read_csv(\"../Dataset/superSurvey_clean_test_data.csv\", index_col=0)\n",
    "\n",
    "# transform data\n",
    "train_x, train_y, train_N, train_theta = transform_data(train_data)\n",
    "test_x, test_y, test_N, test_theta = transform_data(test_data)\n",
    "\n",
    "# transfers the PyTorch tensors to GPU\n",
    "train_x, train_y, train_N, train_theta = train_x.to(device), train_y.to(device), train_N.to(device), train_theta.to(device)\n",
    "test_x, test_y, test_N, test_theta = test_x.to(device), test_y.to(device), test_N.to(device), test_theta.to(device)\n",
    "\n",
    "# normalize data\n",
    "train_x = (train_x - train_x.mean(0)) / train_x.std(0)\n",
    "test_x = (test_x - test_x.mean(0)) / test_x.std(0)\n",
    "\n",
    "# initialize likelihood and GP model\n",
    "likelihood = BinomialLikelihood(n_trials=train_N).to(device)\n",
    "model = BinomialGPModel(train_x=train_x).float().to(device)\n",
    "\n",
    "training_iterations = 200\n",
    "\n",
    "### Hyperparameter ###\n",
    "# initialize the lengthscale hyperparameters of the GP's covariance function\n",
    "# (lengthscale determines how much influence each input feature has on the GP's prediction)\n",
    "# train_x.std(0, unbiased=False) computes the SD of each feature (column) in the training data\n",
    "# unbiased=False ensures that the SD is calculated using the population formula (denominator n)\n",
    "# rather than the sample formula (denominator n-1)\n",
    "# 1 / train_x.std(0, unbiased=False) inverts the SD to set the initial lengthscales\n",
    "# inversely proportional to the variability of each feature\n",
    "# features with higher variability receive smaller initial lengthscales\n",
    "# this indicates they might have a more significant impact on the model's predictions\n",
    "lengthscale_init = 1 / train_x.std(0, unbiased=False)  \n",
    "\n",
    "# calculate the proportion of successes (y) out of total trials (n) for each training sample\n",
    "y_over_n = train_y / train_N\n",
    "\n",
    "# clamp values to be within (0,1) since the inverse CDF is undefined at 0 and 1\n",
    "y_over_n_clamped = y_over_n.clamp(min=0.001, max=0.999)\n",
    "\n",
    "# transform the clamped proportion using the inverse CDF of the standard normal distribution \n",
    "# this maps the proportions to the latent variable space assumed by the GP model\n",
    "theta_transformed = torch.distributions.Normal(0, 1).icdf(y_over_n_clamped)\n",
    "\n",
    "# calculate the SD of the transformed data to set the inital output scale\n",
    "output_scale_init = theta_transformed.std(unbiased=False).item()\n",
    "\n",
    "# initialize hyperparameters\n",
    "hypers = {\n",
    "    'mean_module.bias': torch.tensor(0.),\n",
    "    'covar_module.base_kernel.lengthscale': lengthscale_init,\n",
    "    'covar_module.outputscale': torch.tensor(output_scale_init),\n",
    "}\n",
    "model.initialize(**hypers)\n",
    "\n",
    "# put both the GP model and likelihood into training mode\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# initialize Adam optimizer to update model's parameters based on\n",
    "# computed gradients during training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# define the loss function (ELBO)\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, train_y.numel())\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i in range(training_iterations):\n",
    "    # clear previously accumulated gradients to prevent them from being\n",
    "    # mixed with the current iteration's gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # passes the training input features through the GP model to obtain the\n",
    "    # predictive output\n",
    "    output = model(train_x)\n",
    "    \n",
    "    # calculate the negative ELBO by evaluating the mll (variational ELBO)\n",
    "    # on the model's output and the target variable train_y\n",
    "    # minimizing negative ELBO = maximizing ELBO\n",
    "    # this balances the data fit and model complexity \n",
    "    loss = -mll(output, train_y)\n",
    "\n",
    "    # compute the gradients of the loss with respect to the model's parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update model's parameters on the computed gradients and the Adam's update rule\n",
    "    optimizer.step()\n",
    "    \n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Training completed in {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.3737, 2.3744, 1.4577, 1.4652, 1.9657, 0.7110]], device='cuda:0',\n",
       "       grad_fn=<SoftplusBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.covar_module.base_kernel.lengthscale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'training data')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    f_pred = model(train_x)\n",
    "    mu = f_pred.mean.cpu().numpy()\n",
    "\n",
    "if torch.is_tensor(train_theta) and train_theta.is_cuda:\n",
    "    train_theta = train_theta.cpu().numpy()\n",
    "elif not isinstance(train_theta, np.ndarray):\n",
    "    train_theta = np.array(train_theta)\n",
    "\n",
    "# calculate CDFs for test data\n",
    "def calculate_cdf(data):\n",
    "    sorted_data = np.sort(data)\n",
    "    cdf = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "    return sorted_data, cdf\n",
    "    \n",
    "sorted_train_theta, cdf_train_theta = calculate_cdf(train_theta)\n",
    "sorted_mu, cdf_mu = calculate_cdf(mu)\n",
    "\n",
    "# plotting the CDFs for test data\n",
    "plt.scatter(sorted_train_theta, cdf_mu)\n",
    "plt.xlabel(\"actual theta\")\n",
    "plt.ylabel(\"cdf est theta\")\n",
    "plt.title(\"training data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'testing data')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    f_pred2 = model(test_x)\n",
    "    mu2 = f_pred2.mean.cpu().numpy()\n",
    "\n",
    "if torch.is_tensor(test_theta) and test_theta.is_cuda:\n",
    "    test_theta = test_theta.cpu().numpy()\n",
    "elif not isinstance(test_theta, np.ndarray):\n",
    "    test_theta = np.array(test_theta)\n",
    "\n",
    "# calculate CDFs for test data\n",
    "sorted_test_theta, cdf_test_theta = calculate_cdf(test_theta)\n",
    "sorted_mu2, cdf_mu2 = calculate_cdf(mu2)\n",
    "\n",
    "# plotting the CDFs for test data\n",
    "plt.scatter(sorted_test_theta,cdf_mu2)\n",
    "plt.xlabel(\"actual theta\")\n",
    "plt.ylabel(\"cdf est theta\")\n",
    "plt.title(\"testing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZpldSvn8jgp",
    "outputId": "c1234c2e-5c08-4318-c0ee-3b00ace23c4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.csv  Model_running.ipynb  rho.csv\ttest_data.csv  train_data.csv\n"
     ]
    }
   ],
   "source": [
    "# !ls /content/drive/My\\ Drive/Colab\\ Notebooks/Model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Xwo6zwXh8jvH"
   },
   "outputs": [],
   "source": [
    "# # path for saving\n",
    "# model_path = \"/home/alexis/Downloads/superSurvey_model.pth\"\n",
    "# optimizer_path = \"/home/alexis/Downloads/superSurvey_optimizer.pth\"\n",
    "\n",
    "# # save the model state\n",
    "# torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# # save the optimizer state\n",
    "# torch.save(optimizer.state_dict(), optimizer_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
